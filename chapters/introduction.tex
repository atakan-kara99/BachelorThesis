\chapter{Introduction}

In the ever-evolving landscape of data analysis and machine learning, dimensionality reduction techniques have emerged as invaluable tools for transforming high-dimensional data into a more manageable and interpretable form. These dimensionality reduction techniques can be subdivided into linear (e.g. PCA) and non-linear ones, the latter also prominent among the literature by the term "manifold learning" and the main focus of our work. Manifold learning methods have gained substantial attention due to their ability to capture the intrinsic structure and relationships within more complex datasets that do not just consist of linear relations but also of non-linear types that provide a closer approximation of the underlying/generating distributions in real-world-oriented datasets. These methods enable the preservation of essential properties like distances (global) and neighborhood (local) structures while reducing the dimensionality, making them particularly useful in data science, including data mining, data analysis, and data visualization in various fields such as image processing, speech recognition, genomics, finance, and climatology.

The high dimensionality of data complicates their analysis and often makes it difficult to identify complex patterns and relationships in the data that has its origin in the curse of dimensionality.\cite{curse_dim} Manifold learning algorithms are a promising approach to address this challenge. They not only facilitate the analysis of data but they reduce the computational complexity of these analysis methods. In most cases the intrinsic and the original dimensionality do not match, meaning that substantial amounts of redundancies do prevail in datasets that are not relevant for analysis. Another important utility of manifold learning methods is to get meaningful low-dimensional data that can be visualized to facilitate data analysis and interpretation.

The manifold learning techniques we will consider in our thesis are Multi-Dimensional Scaling (MDS), Locally Linear Embedding (LLE), and t-Distributed Stochastic Neighbor Embedding (t-SNE). They have demonstrated their effectiveness in numerous applications exhibiting different strengths and weaknesses. However, understanding the nuanced differences in their performance is crucial, especially when applied to different types of datasets. To address this, we undertake an analysis and comparison of these methods, focusing on their capability to preserve distances and neighborhood structures, as well as their utility in visualizing and clustering data.

Three diverse datasets, the 3D-Mammoth a transformed version of it, and the COIL-20 dataset, serve as the testing grounds for our investigation. These datasets have been selected to represent distinct challenges and characteristics commonly encountered in real-world data analysis scenarios. The 3D-Mammoth dataset provides us with three-dimensional spatial information from a real Mammoth where the transformed version is a non-continuous copy of it. On the other hand, the COIL-20 dataset consists of 2D images, presenting issues such as high dimensionality and intrinsic structure embedded within images.

In this thesis, our primary objectives are to explore the suitability of manifold learning methods for various dataset types, tasks, and objectives and to discern the factors that lead to challenges and limitations in their application. We hope to provide insights into the optimal selection and configuration of these methods. Furthermore, we are interested in how the parameter settings of the methods intervene in these questions with the superordinate goal of getting the most stable outcome.

In the subsequent chapters, we therefore delve into related work that deals with similar questions and provide an overview of each method, including their underlying principles and mathematical foundations. We will also discuss the limitations of these techniques, offering insights into their practical use cases. We will then introduce our experimental setup, which enables a systematic comparison of our manifold learning methods deployed on the datasets. With the resulting outcomes of our analysis, we intend to provide a resource for data scientists and researchers seeking guidance on selecting the most suitable manifold learning method for their specific data analysis tasks. Lastly, we will discuss our results and list some more questions that arose while doing our experiments that could be part of some future work. Through this research, we hope to contribute to the ongoing exploration of manifold learning techniques, advancing our understanding of their capabilities and limitations in real-world applications. Note that in the following the abbreviations DR and NLDR are used which stand for (non-linear) dimensionality reduction.